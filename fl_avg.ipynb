{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import torchvision\n",
    "import re\n",
    "import numpy\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(torch.nn.Module):\n",
    "    def __init__(self, input_channels):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=input_channels, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.pool1 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        self.pool2 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = torch.nn.Linear(in_features=16 * 4 * 4, out_features=120)\n",
    "        self.fc2 = torch.nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = torch.nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(torch.relu(self.conv1(x)))\n",
    "        x = self.pool2(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Loss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return torch.nn.functional.cross_entropy(input, target)\n",
    "\n",
    "\n",
    "class Server:\n",
    "    def __init__(self, model, client_params):\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.client_params = client_params\n",
    "        self.n_client = len(self.client_params)\n",
    "\n",
    "        self.server_params = self.client_params[0]\n",
    "        for key in self.server_params:\n",
    "            self.server_params[key] = self.server_params[key].div(\n",
    "                self.n_client)\n",
    "\n",
    "    def fed_avg(self):\n",
    "        for client in range(self.n_client):\n",
    "            for key in self.server_params:\n",
    "                deal_param = self.client_params[client][key].div(self.n_client)\n",
    "                self.server_params[key] = self.server_params[key].add(deal_param)\n",
    "        return self.server_params\n",
    "\n",
    "\n",
    "class DealDataset(Dataset):\n",
    "    def __init__(self, dataset, idx):\n",
    "        self.dataset = dataset\n",
    "        self.idx = idx\n",
    "        self.len = len(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.dataset[self.idx[index]]\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idx_split(dataset, mode='iid', n_dataset=1, n_data_each_set=1):\n",
    "    labels_list = dataset.targets.tolist()\n",
    "    all_labels = set(labels_list)\n",
    "    idx_label = dict()\n",
    "    for label in all_labels:\n",
    "        idx_label[label] = list()\n",
    "        for idx, label_in_list in enumerate(labels_list):\n",
    "            if label_in_list == label:\n",
    "                idx_label[label] += [idx]\n",
    "\n",
    "    if mode == 'iid':\n",
    "        n_each_set = dict()\n",
    "        for label in all_labels:\n",
    "            n_each_set[label] = int(\n",
    "                len(idx_label[label]) / len(labels_list) * n_data_each_set / n_dataset)\n",
    "            print(label, n_each_set[label], end='|')\n",
    "        dataset_splited = dict()\n",
    "        left_idx_label = idx_label\n",
    "        for i in range(n_dataset):\n",
    "            dataset_splited[i] = list()\n",
    "            for label in all_labels:\n",
    "                choiced_idx = numpy.random.choice(\n",
    "                    left_idx_label[label],\n",
    "                    n_each_set[label],\n",
    "                    replace=False)\n",
    "                dataset_splited[i] += list(choiced_idx)\n",
    "                left_idx_label[i] = list(\n",
    "                    set(left_idx_label[label]) - set(dataset_splited[i]))\n",
    "                print('\\n', len(left_idx_label[i]))\n",
    "        return dataset_splited\n",
    "    elif mode == 'non-iid':\n",
    "        print('TO DO.')\n",
    "\n",
    "\n",
    "def train_model(model, dataset, device='cpu', epochs=1):\n",
    "    trained_model = copy.deepcopy(model).to(device)\n",
    "    trained_model.train()\n",
    "    train_dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(trained_model.parameters())\n",
    "    for epoch in range(epochs):\n",
    "        for i, (data, label) in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            output = trained_model(data.to(device))\n",
    "            loss = criterion(output, label.to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        #     if (i+1) % 100 == 0:\n",
    "        #         print('\\r', end='')\n",
    "        #         print(\n",
    "        #             f'step [{i+1}/{len(train_dataloader)}], loss: {loss.item():.4f}', end='')\n",
    "        # print(f'\\nepoch {epoch+1}/{epochs} down.')\n",
    "    return trained_model\n",
    "\n",
    "\n",
    "def eval_model(model, dataset):\n",
    "    server_model = copy.deepcopy(model)\n",
    "    server_model.eval()\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        data_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "        for images, labels in data_loader:\n",
    "            outputs = server_model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        # print('Test Accuracy: {:.2f}%'.format(100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "0 98|1 112|2 99|3 102|4 97|5 90|6 98|7 104|8 97|9 99|\n",
      " 5825\n",
      "\n",
      " 6630\n",
      "\n",
      " 5859\n",
      "\n",
      " 6029\n",
      "\n",
      " 5745\n",
      "\n",
      " 5331\n",
      "\n",
      " 5820\n",
      "\n",
      " 6161\n",
      "\n",
      " 5754\n",
      "\n",
      " 5850\n",
      "\n",
      " 5752\n",
      "\n",
      " 5640\n",
      "\n",
      " 5859\n",
      "\n",
      " 6029\n",
      "\n",
      " 5745\n",
      "\n",
      " 5331\n",
      "\n",
      " 5820\n",
      "\n",
      " 6161\n",
      "\n",
      " 5754\n",
      "\n",
      " 5642\n",
      "\n",
      " 5752\n",
      "\n",
      " 5443\n",
      "\n",
      " 5344\n",
      "\n",
      " 6029\n",
      "\n",
      " 5745\n",
      "\n",
      " 5331\n",
      "\n",
      " 5820\n",
      "\n",
      " 6161\n",
      "\n",
      " 5754\n",
      "\n",
      " 5548\n",
      "\n",
      " 5752\n",
      "\n",
      " 5438\n",
      "\n",
      " 5255\n",
      "\n",
      " 5153\n",
      "\n",
      " 5745\n",
      "\n",
      " 5331\n",
      "\n",
      " 5820\n",
      "\n",
      " 6161\n",
      "\n",
      " 5754\n",
      "\n",
      " 5453\n",
      "\n",
      " 5752\n",
      "\n",
      " 5439\n",
      "\n",
      " 5256\n",
      "\n",
      " 5079\n",
      "\n",
      " 4982\n",
      "\n",
      " 5331\n",
      "\n",
      " 5820\n",
      "\n",
      " 6161\n",
      "\n",
      " 5754\n",
      "\n",
      " 5361\n",
      "\n",
      " 5752\n",
      "\n",
      " 5436\n",
      "\n",
      " 5256\n",
      "\n",
      " 5073\n",
      "\n",
      " 4909\n",
      "\n",
      " 4819\n",
      "\n",
      " 5820\n",
      "\n",
      " 6161\n",
      "\n",
      " 5754\n",
      "\n",
      " 5279\n",
      "\n",
      " 5752\n",
      "\n",
      " 5439\n",
      "\n",
      " 5252\n",
      "\n",
      " 5077\n",
      "\n",
      " 4916\n",
      "\n",
      " 4758\n",
      "\n",
      " 4660\n",
      "\n",
      " 6161\n",
      "\n",
      " 5754\n",
      "\n",
      " 5192\n",
      "\n",
      " 5752\n",
      "\n",
      " 5440\n",
      "\n",
      " 5252\n",
      "\n",
      " 5076\n",
      "\n",
      " 4905\n",
      "\n",
      " 4770\n",
      "\n",
      " 4618\n",
      "\n",
      " 4514\n",
      "\n",
      " 5754\n",
      "\n",
      " 5094\n",
      "\n",
      " 5752\n",
      "\n",
      " 5439\n",
      "\n",
      " 5263\n",
      "\n",
      " 5079\n",
      "\n",
      " 4908\n",
      "\n",
      " 4768\n",
      "\n",
      " 4616\n",
      "\n",
      " 4438\n",
      "\n",
      " 4341\n",
      "\n",
      " 5023\n",
      "\n",
      " 5752\n",
      "\n",
      " 5439\n",
      "\n",
      " 5268\n",
      "\n",
      " 5071\n",
      "\n",
      " 4901\n",
      "\n",
      " 4761\n",
      "\n",
      " 4618\n",
      "\n",
      " 4414\n",
      "\n",
      " 4303\n",
      "\n",
      " 4204\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "print(device)\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "n_total_client = 10\n",
    "n_data = 10000\n",
    "communication_round = 1\n",
    "epochs = 1\n",
    "n_client = 3\n",
    "\n",
    "model = LeNet5(input_channels=1)\n",
    "idx_splited = idx_split(dataset=train_dataset,\n",
    "                        n_dataset=n_total_client,\n",
    "                        n_data_each_set=n_data)\n",
    "dataset_client = dict()\n",
    "for i in range(n_total_client):\n",
    "    dataset_client[i] = DealDataset(train_dataset, idx_splited[i])\n",
    "\n",
    "server_model = model\n",
    "for i in range(communication_round):\n",
    "    client = dict()\n",
    "    choicen_client = numpy.random.choice(\n",
    "        range(n_total_client), n_client, replace=False)\n",
    "    for j, k in enumerate(choicen_client):\n",
    "        client[j] = train_model(\n",
    "            model=server_model,\n",
    "            dataset=DealDataset(train_dataset, idx_splited[k]),\n",
    "            device=device,\n",
    "            epochs=epochs).state_dict()\n",
    "    server_model = Server(model=model, client_params=client)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
