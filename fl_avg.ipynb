{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import tqdm\n",
    "import torchvision\n",
    "import re\n",
    "import numpy\n",
    "import copy\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet5(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = torch.nn.Conv2d(\n",
    "            in_channels=1, out_channels=6, kernel_size=5, stride=1)\n",
    "        self.pool1 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = torch.nn.Conv2d(\n",
    "            in_channels=6, out_channels=16, kernel_size=5, stride=1)\n",
    "        self.pool2 = torch.nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.fc1 = torch.nn.Linear(in_features=16 * 4 * 4, out_features=120)\n",
    "        self.fc2 = torch.nn.Linear(in_features=120, out_features=84)\n",
    "        self.fc3 = torch.nn.Linear(in_features=84, out_features=10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool1(torch.relu(self.conv1(x)))\n",
    "        x = self.pool2(torch.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 4 * 4)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class Loss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        return torch.nn.functional.cross_entropy(input, target)\n",
    "\n",
    "\n",
    "class Client:\n",
    "    def __init__(self, model, dataloader, optimizer='adam', device='cpu', epochs=1, loss=Loss()):\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.dataloader = dataloader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        # self.loss = loss\n",
    "        self.loss = torch.nn.CrossEntropyLoss()\n",
    "        if optimizer == 'adam':\n",
    "            self.optimizer = torch.optim.Adam(self.model.parameters())\n",
    "        else:\n",
    "            self.optimizer = torch.optim.SGD(self.model.parameters())\n",
    "\n",
    "    def train(self):\n",
    "        self.model.train()\n",
    "        self.model.to(self.device)\n",
    "        for epoch in range(self.epochs):\n",
    "            for i, (data, label) in enumerate(self.dataloader):\n",
    "                self.optimizer.zero_grad()\n",
    "                output = self.model(data.to(self.device))\n",
    "                loss = self.loss(output, label.to(self.device))\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                if (i+1) % 100 == 0:\n",
    "                    print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, self.epochs, i+1, len(self.dataloader), loss.item()))\n",
    "        return self.model.state_dict()\n",
    "\n",
    "\n",
    "class Server:\n",
    "    def __init__(self, model, client_params):\n",
    "        self.model = copy.deepcopy(model)\n",
    "        self.client_params = client_params\n",
    "        self.n_client = len(self.client_params)\n",
    "\n",
    "        self.clients = list(client_params.keys())\n",
    "        self.server_params = self.client_params[self.clients[0]]\n",
    "        for key in self.server_params:\n",
    "            self.server_params[key] = self.server_params[key].div(self.n_client)\n",
    "\n",
    "    def fed_avg(self):\n",
    "        for client in self.clients:\n",
    "            for key in self.server_params:\n",
    "                self.server_params[key] = self.server_params[key].add(self.client_params[client][key].div(self.n_client))\n",
    "        return self.server_params\n",
    "\n",
    "\n",
    "class DealDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dataset, idx):\n",
    "        self.dataset = dataset\n",
    "        self.idx = idx\n",
    "        self.len = len(idx)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img, target = self.dataset[self.idx[index]]\n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DateSplit(dataset, mode='iid', n_dataset=1, n_data_each_set=1):\n",
    "    if mode == 'iid':\n",
    "        labels_list = dataset.targets.tolist()\n",
    "        all_labels = set(labels_list)\n",
    "        idx_label = dict()\n",
    "        for label in all_labels:\n",
    "            idx_label[label] = list([\n",
    "                idx for idx, _ in enumerate(labels_list) if labels_list[idx] == label])\n",
    "        dataset_splited = dict()\n",
    "        for i in range(n_dataset):\n",
    "            dataset_splited[i] = list()\n",
    "            for label in all_labels:\n",
    "                choiced_idx = numpy.random.choice(idx_label[label], n_data_each_set, replace=False)\n",
    "                dataset_splited[i] += list(choiced_idx)\n",
    "        return dataset_splited\n",
    "    elif mode == 'non-iid':\n",
    "        print('TO DO.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.0353\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.2582\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.4983\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.6016\n",
      "Epoch [1/1], Step [900/1000], Loss: 1.3556\n",
      "Epoch [1/1], Step [1000/1000], Loss: 2.1414\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [500/1000], Loss: 1.0144\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.4892\n",
      "Epoch [1/1], Step [700/1000], Loss: 1.3386\n",
      "Epoch [1/1], Step [800/1000], Loss: 1.1008\n",
      "Epoch [1/1], Step [900/1000], Loss: 1.1848\n",
      "Epoch [1/1], Step [1000/1000], Loss: 1.8044\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.1240\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.0788\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.5805\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.0610\n",
      "Epoch [1/1], Step [900/1000], Loss: 1.4995\n",
      "Epoch [1/1], Step [1000/1000], Loss: 1.8649\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0002\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0012\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.0072\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.0311\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.0160\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.0224\n",
      "Epoch [1/1], Step [900/1000], Loss: 0.1394\n",
      "Epoch [1/1], Step [1000/1000], Loss: 0.8510\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0001\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.1090\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.1398\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.0872\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.2609\n",
      "Epoch [1/1], Step [900/1000], Loss: 1.0176\n",
      "Epoch [1/1], Step [1000/1000], Loss: 1.9533\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.1341\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.6456\n",
      "Epoch [1/1], Step [700/1000], Loss: 1.2805\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.9305\n",
      "Epoch [1/1], Step [900/1000], Loss: 1.0533\n",
      "Epoch [1/1], Step [1000/1000], Loss: 0.7995\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0001\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.4832\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.0689\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.9906\n",
      "Epoch [1/1], Step [800/1000], Loss: 1.6681\n",
      "Epoch [1/1], Step [900/1000], Loss: 0.3999\n",
      "Epoch [1/1], Step [1000/1000], Loss: 1.6157\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0001\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.2583\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.4388\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.3003\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.4146\n",
      "Epoch [1/1], Step [900/1000], Loss: 1.1847\n",
      "Epoch [1/1], Step [1000/1000], Loss: 0.9943\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.1068\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.8019\n",
      "Epoch [1/1], Step [700/1000], Loss: 1.4898\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.0606\n",
      "Epoch [1/1], Step [900/1000], Loss: 0.1828\n",
      "Epoch [1/1], Step [1000/1000], Loss: 0.4314\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.0187\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.0394\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.1718\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.9628\n",
      "Epoch [1/1], Step [900/1000], Loss: 0.8909\n",
      "Epoch [1/1], Step [1000/1000], Loss: 1.5501\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0120\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0318\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.0545\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.6176\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.0207\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.1182\n",
      "Epoch [1/1], Step [900/1000], Loss: 0.6536\n",
      "Epoch [1/1], Step [1000/1000], Loss: 1.2091\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0344\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0341\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.0001\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.2241\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.0890\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.0923\n",
      "Epoch [1/1], Step [900/1000], Loss: 0.0236\n",
      "Epoch [1/1], Step [1000/1000], Loss: 1.4474\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0165\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0028\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0849\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.0011\n",
      "Epoch [1/1], Step [600/1000], Loss: 1.4108\n",
      "Epoch [1/1], Step [700/1000], Loss: 1.7474\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.0531\n",
      "Epoch [1/1], Step [900/1000], Loss: 0.2855\n",
      "Epoch [1/1], Step [1000/1000], Loss: 0.4589\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0068\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0001\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0757\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.0285\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.0147\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.0538\n",
      "Epoch [1/1], Step [900/1000], Loss: 0.0279\n",
      "Epoch [1/1], Step [1000/1000], Loss: 0.8056\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0233\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0050\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.0012\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.1645\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.3712\n",
      "Epoch [1/1], Step [800/1000], Loss: 1.6237\n",
      "Epoch [1/1], Step [900/1000], Loss: 0.0866\n",
      "Epoch [1/1], Step [1000/1000], Loss: 1.0662\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0489\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0002\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.0417\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.0058\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.0050\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.0305\n",
      "Epoch [1/1], Step [900/1000], Loss: 0.1742\n",
      "Epoch [1/1], Step [1000/1000], Loss: 1.6517\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0001\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0002\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0363\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.0002\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.0139\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.0166\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.0905\n",
      "Epoch [1/1], Step [900/1000], Loss: 0.0463\n",
      "Epoch [1/1], Step [1000/1000], Loss: 0.1098\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0441\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0032\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0008\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [600/1000], Loss: 1.1333\n",
      "Epoch [1/1], Step [700/1000], Loss: 1.2879\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.0428\n",
      "Epoch [1/1], Step [900/1000], Loss: 0.0409\n",
      "Epoch [1/1], Step [1000/1000], Loss: 0.6760\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0074\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0048\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.1429\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.0010\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.3012\n",
      "Epoch [1/1], Step [800/1000], Loss: 1.3264\n",
      "Epoch [1/1], Step [900/1000], Loss: 1.1716\n",
      "Epoch [1/1], Step [1000/1000], Loss: 1.8279\n",
      "Epoch [1/1], Step [100/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [200/1000], Loss: 0.0177\n",
      "Epoch [1/1], Step [300/1000], Loss: 0.0000\n",
      "Epoch [1/1], Step [400/1000], Loss: 0.0337\n",
      "Epoch [1/1], Step [500/1000], Loss: 0.1654\n",
      "Epoch [1/1], Step [600/1000], Loss: 0.6994\n",
      "Epoch [1/1], Step [700/1000], Loss: 0.0521\n",
      "Epoch [1/1], Step [800/1000], Loss: 0.0851\n",
      "Epoch [1/1], Step [900/1000], Loss: 0.8488\n",
      "Epoch [1/1], Step [1000/1000], Loss: 1.3611\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    "    download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root='./data',\n",
    "    train=False,\n",
    "    transform=torchvision.transforms.ToTensor())\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model = LeNet5()\n",
    "criterion = Loss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "n_client = 10\n",
    "n_data = 3200\n",
    "idx_splited = DateSplit(dataset=train_dataset,\n",
    "                        n_dataset=n_client,\n",
    "                        n_data_each_set=n_data)\n",
    "\n",
    "choice_client = 10\n",
    "conmunication_rounds = 2\n",
    "\n",
    "server_params = copy.deepcopy(model).state_dict()\n",
    "for i in range(conmunication_rounds):\n",
    "    client_params = dict()\n",
    "    for client in list(numpy.random.choice(range(n_client), choice_client, replace=False)):\n",
    "        client_model = copy.deepcopy(model)\n",
    "        client_model.load_state_dict(server_params)\n",
    "        client_params[client] = Client(model=client_model,\n",
    "                            dataloader = DataLoader(\n",
    "                                DealDataset(train_dataset,\n",
    "                                            idx_splited[client]),\n",
    "                                            batch_size=32,\n",
    "                                            shuffle=False\n",
    "                                            ),\n",
    "                                optimizer='adam',\n",
    "                                device=device).train()\n",
    "    server_params = Server(model=model, client_params=client_params).fed_avg()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 10.09%\n"
     ]
    }
   ],
   "source": [
    "server_model = copy.deepcopy(model)\n",
    "server_model.load_state_dict(server_params)\n",
    "server_model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = server_model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy: {:.2f}%'.format(100 * correct / total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
